{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Fugue Does NOT Want To Be Another Pandas-Like Framework\n",
    "\n",
    "Fugue fully utilizes Pandas for computing tasks, but **Fugue is NOT a Pandas-like computing framework, and it never wants to be.** In this article we are going to explain the reason for this critical design decision.\n",
    "\n",
    "## Benchmarking PySpark Pandas (Koalas)\n",
    "\n",
    "This is an example modified from a real piece of Pandas user code. Assume we have Pandas dataframes generated by this code:\n",
    "\n",
    "```python\n",
    "def gen(n):\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(dict(\n",
    "        a=np.random.choice([\"aa\",\"abcd\",\"xyzzzz\",\"tttfs\"],n),\n",
    "        b=np.random.randint(0,100,n),\n",
    "        c=np.random.choice([\"aa\",\"abcd\",\"xyzzzz\",\"tttfs\"],n),\n",
    "        d=np.random.randint(0,10000,n),\n",
    "    ))\n",
    "```\n",
    "\n",
    "The output has four columns with string and integer types. Here is the user's code:\n",
    "\n",
    "```python\n",
    "df.sort_values([\"a\", \"b\", \"c\", \"d\"]).drop_duplicates(subset=[\"a\", \"b\"], keep=\"last\")\n",
    "```\n",
    "\n",
    "Based on the code, the user want to firstly partition the dataframe by `a` and `b`,\n",
    "and in each group, the user wants to sort by `c` and `d` and then to get the last record\n",
    "of each group.\n",
    "\n",
    "\n",
    "### Configuration and Datasets\n",
    "\n",
    "* **Databricks runtime version:** 10.1 (Scala 2.12 Spark 3.2..0)\n",
    "* **Cluster:** 1 i3.xlarge driver instance and 8 i3.xlarge worker instances\n",
    "\n",
    "And we will use 4 different datasets: 1 million, 10 million, 20 million, and 30 million\n",
    "\n",
    "```python\n",
    "g1 = gen(1 * 1000 * 1000)\n",
    "df1 = spark.createDataFrame(g1).cache()\n",
    "df1.count()\n",
    "pdf1 = df1.to_pandas_on_spark()\n",
    "\n",
    "g10 = gen(10 * 1000 * 1000)\n",
    "df10 = spark.createDataFrame(g10).cache()\n",
    "df10.count()\n",
    "pdf10 = df10.to_pandas_on_spark()\n",
    "\n",
    "g20 = gen(20 * 1000 * 1000)\n",
    "df20 = spark.createDataFrame(g20).cache()\n",
    "df20.count()\n",
    "pdf20 = df20.to_pandas_on_spark()\n",
    "\n",
    "g30 = gen(30 * 1000 * 1000)\n",
    "df30 = spark.createDataFrame(g30).cache()\n",
    "df30.count()\n",
    "pdf30 = df30.to_pandas_on_spark()\n",
    "```\n",
    "\n",
    "### Comparison 1\n",
    "\n",
    "Let's firstly follow the user's original logic, and we will discuss the alternative solution later.\n",
    "\n",
    "In this [Databrick's article](https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html)\n",
    "the author claimed that Pandas users will be able to scale their workloads with one simple line change in the Spark 3.2 release.\n",
    "So we will first convert the Pandas dataframe to the Spark Pandas dataframe (and without any other change) to verify the result.\n",
    "\n",
    "On the other hand, in traditional Spark, a [window function solution](https://stackoverflow.com/a/33878701) is typical.\n",
    "So we will also add the window function solution to the comparison.\n",
    "\n",
    "To force the full execution of the statement and also to verify result consistency, at the end of each execution\n",
    "we will compute the sum of column `d` and print.\n",
    "\n",
    "Based on the output, the 3 solutions all have consistent result, meaning they have no correctness issue, now let's\n",
    "take a look at their speed:\n",
    "\n",
    "![Sort Dedup vs Window](../../images/pandas_like_1.png)\n",
    "\n",
    "* With a 32 core Spark cluster, both Spark solutions are significantly faster than\n",
    "  the single core Pandas solution\n",
    "* The window function solution is 30% to 50% faster than the Spark Pandas solution\n",
    "\n",
    "On a local machine, a global sort is a very popular technique that is often seen in Pandas code. And in certain\n",
    "scenarios it outperforms other methods. However\n",
    "the global sort operation in distributed computing is difficult and expensive. The performance depends on each\n",
    "specific computing framework's implementation. Spark Pandas has done an amazing job, but even so,\n",
    "it is still significantly slower than a window function.\n",
    "\n",
    "Rethinking about the problem we want to solve, a global sort on the entire dataset is not necessary.\n",
    "If convenience is the only thing important, then switching the Pandas backend to Spark Pandas may make sense.\n",
    "However the whole point of moving to Spark is to be more scalable and performant. Moving to window function that\n",
    "will sort inside each partition isn't overly complicated, but the performance advantage is significant.\n",
    "\n",
    "### Comparison 2\n",
    "\n",
    "In the second comparison, we simplify the original problem to not consider column `c`. We only need to remove\n",
    "`c` in `sort_values` to accommodate the change\n",
    "\n",
    "```python\n",
    "df.sort_values([\"a\", \"b\", \"d\"]).drop_duplicates(subset=[\"a\", \"b\"], keep=\"last\")\n",
    "```\n",
    "\n",
    "Again, it's intuitive and convenient and Spark Pandas can inherit this change too. However, this new problem\n",
    "actually means we want to group by `a` and `b` and get the max value of `d`. It can be a simple aggregation\n",
    "in big data. So in this comparison, we add the simple Spark aggregation approach.\n",
    "\n",
    "![Sort Dedup vs Window vs Aggregation](../../images/pandas_like_2.png)\n",
    "\n",
    "* The previous performance pattern stays the same\n",
    "* Spark aggregation takes ~1 sec regardless of data size\n",
    "\n",
    "So now, do you want to just remove column `c` for simplicity or do you want to rewrite the logic for performance?\n",
    "\n",
    "### Comparison 3\n",
    "\n",
    "Let's go back to the original logic where we still have 4 columns. By understanding the intention, we can have an alternative Pandas solution:\n",
    "\n",
    "```python\n",
    "df.groupby([\"a\", \"b\"]).apply(lambda df:df.sort_values([\"c\", \"d\"], ascending=[False, False]).head(1))\n",
    "```\n",
    "\n",
    "When testing on the 1 million dataset, the original logic takes `1.43 sec` while this new logic takes `2.2 sec`. This is probably\n",
    "one of the reasons the user chose the sort & dedup approach. On a small local dataset, a global sort seems to be faster.\n",
    "\n",
    "In this section, we are going to compare groupby-apply with sort-dedup on all datasets. In addition, this fits nicely\n",
    "with [Pandas UDF](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html) scenarios, so we will also\n",
    "compare with the Pandas UDF approach.\n",
    "\n",
    "To avoid duplication, we extract the lambda function:\n",
    "\n",
    "```python\n",
    "def largest(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.sort_values([\"c\",\"d\"], ascending=[False, False]).head(1)\n",
    "```\n",
    "\n",
    "Unfortunately, the first issue we encounter is that Spark Pandas can't take this function\n",
    "\n",
    "```python\n",
    "g1.groupby([\"a\", \"b\"]).apply(largest)  # g1 is a pandas dataframe, it works\n",
    "df1.groupby([\"a\", \"b\"]).applyInPandas(largest, schema=\"a string,b long,c string,d long\")  # Pandas UDF works\n",
    "pdf1.groupby([\"a\", \"b\"]).apply(largest)  # pdf1 is a spark pandas dataframe, it doesn't work\n",
    "```\n",
    "\n",
    "So for Spark Pandas we will need to use:\n",
    "\n",
    "```python\n",
    "pdf1.groupby([\"a\", \"b\"]).apply(lambda df: largest(df))\n",
    "```\n",
    "\n",
    "This breaks the claim that with an import change everything works out of the box.\n",
    "\n",
    "Now let's see the performance chart:\n",
    "\n",
    "![Sort Dedup vs Group Apply vs Pandas UDF](../../images/pandas_like_3.png)\n",
    "\n",
    "* For Pandas, when data size increases, groupby-apply has more performance advantage over sort-dedup\n",
    "* For Spark Pandas, groupby-apply is even slower than Pandas\n",
    "* Pandas UDF is the fastest Spark solution for this problem\n",
    "\n",
    "### Summary of Comparisons\n",
    "\n",
    "With the 3 comparisons we find out:\n",
    "\n",
    "* The convenience is at the cost of performance\n",
    "* Simply switching backend doesn't always work (not 100% consistent)\n",
    "* Simply switching backend can cause unexpected performance issues\n",
    "* Big data problems require different ways of thinking, users must learn and change their mindset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Issues of PySpark Pandas (Koalas)\n",
    "\n",
    "The promise of PySpark Pandas (Koalas) is that you only need to change [the import line of code](https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html) to bring your code from Pandas to Spark. This promise is, of course, too good to be true. In this section we will show some common operations that don't behave as expected. Some of these might be fixable, but some of them are also inherent to the differences of Pandas and Spark.\n",
    "\n",
    "From here on, we'll use the word Koalas to refer to PySpark Pandas to easily distinguish it from Pandas.\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, we set-up Koalas and Pandas DataFrames. They will take the same input and we can check the `head()` of the Koalas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     a  b\n0  NaN  1\n1  NaN  2\n2  1.0  3\n3  1.0  4\n4  2.0  5"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "df = pd.DataFrame({\"a\": [None, None, 1, 1, 2, 2], \"b\": [1, 2, 3, 4, 5, 6]})\n",
    "kdf = ps.DataFrame({\"a\": [None, None, 1, 1, 2, 2], \"b\": [1, 2, 3, 4, 5, 6]})\n",
    "kdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation 1 - Reset Index\n",
    "\n",
    "The first operation we'll look at is resetting an index. Note that Spark does not have any such concept. In order to provide a consistent experience to Pandas, the Koalas DataFrame is a Spark DataFrame that has an index added to it. Because it isn't a Spark DataFrame, you need to convert it to a Spark DataFrame to take advantage of Spark code.\n",
    "\n",
    "So let's look at a simple operation, `groupby-max`. First, we do it with Pandas. Note `NULL` values drop by default in Pandas while they are kept in Spark. This is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>b</th>\n    </tr>\n    <tr>\n      <th>a</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     b\na     \n1.0  4\n2.0  6"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"a\").max(\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason though, the same syntax does not work in Koalas. This is a minor issue, but shows that the APIs will not always match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/x5/f4r6ylss0k7dwh8c_0dmzd_40000gn/T/ipykernel_34170/2338264843.py\", line 4, in <module>\n",
      "    kdf.groupby(\"a\").max(\"b\")\n",
      "TypeError: max() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "try:\n",
    "    kdf.groupby(\"a\").max(\"b\")\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we tweak the syntax a bit and it works. This operation is consistent with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>b</th>\n    </tr>\n    <tr>\n      <th>a</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     b\na     \n1.0  4\n2.0  6"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf.groupby(\"a\").max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas, it is very common operation to `reset_index()`. So we add it to the Koalas expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/08 21:40:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/08 21:40:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/08 21:40:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/08 21:40:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     a  b\n0  1.0  4\n1  2.0  6"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf.groupby(\"a\").max().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has a lot of warning messages. This one though can not be easily ignored. It says we did not define a partitioning scheme, so all of the data is being collected to a single machine and then the index is added. This is very expensive and likely to crash for big data. It also defeats the purpose of using a Spark backend. The default `reset_index()` behavior is actually very harmful, and likely to hurt performance compared to if you just used Pandas. This is because there is an overhead to move data round in order to add the index.\n",
    "\n",
    "Pandas relies on the index a lot, while Spark has no concept of index. Thus, a design decision has to be made whether or not to be consistent with Pandas or Spark. Koalas chooses to be consistent with Pandas. We'll look at one more example with the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operation 2 - iloc\n",
    "\n",
    "All of the data in Pandas lives in a single machine. Because of this, indexing it is trivial. What is the index? It is a global sort of the data. When we do `reset_index()`, we number the data from starting from 0 to the last record. Once we do this, we can use the `iloc` operation, or look up by index and we are guaranteed order. A simple Pandas example is below. Everything is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>b</th>\n      <th>a</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   b    a\n0  1  NaN\n1  2  NaN\n2  3  1.0\n3  4  1.0\n4  5  2.0"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"a\": [None, None, 1, 1, 2, 2], \"b\": [1, 2, 3, 4, 5, 6]})\n",
    "res = []\n",
    "for i in range(10):\n",
    "    res.append(df)\n",
    "t = pd.concat(res, axis=0)\n",
    "t = t.groupby(\"b\").min().reset_index()\n",
    "t.iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how doing `iloc` in Pandas gives the values for `b` from 1 to 5 in order. Let's compare this operation in Koalas. For the Koalas version of this operation, you may get consistent records if you run this locally. But when run distributedly on a cluster, you will see inconsistent behavior. Below is a screen shot of a run on Databricks.\n",
    "\n",
    "![Databricks Koalas](../../images/databricks_koalas_iloc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `iloc` values give us a different order of records. This is because order is not guaranteed in Spark, and it's expensive to keep a global location index across multiple machines. This can cause inaccurate results even if code successfully runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining\n",
    "\n",
    "For joining, Pandas joins `NULL` with `NULL` while Spark does not. Let's see who Koalas is consistent with. Recall for `groupby` it was consistent with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.0</td>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     a  b  c\n0  NaN  1  1\n1  NaN  2  1\n2  1.0  3  2\n3  1.0  4  2\n4  2.0  5  3\n5  2.0  6  3"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({\"a\": [None, 1, 2], \"c\": [1, 2, 3]})\n",
    "df.merge(df2, on=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the same operation on Koalas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/08 23:38:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/08 23:38:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/08 23:38:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/08 23:38:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/08 23:38:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     a  b  c\n0  1.0  3  2\n1  1.0  4  2\n2  2.0  5  3\n3  2.0  6  3"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf2 = ps.DataFrame({\"a\": [None, 1, 2], \"c\": [1, 2, 3]})\n",
    "kdf.merge(kdf2, on=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `NULL` values are dropped. This means that for join, Koalas is actually closer to the Spark behavior than Pandas. There is a consistency issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Types in Columns\n",
    "\n",
    "One of the things acceptable in Pandas is having columns that contain different data types. The reason it's hard to execute this behavior in Spark is because Spark operates on different machines, and in order to guaratee consistent behavior across partitions, schema needs to be explicit.\n",
    "\n",
    "Related to this, inferring schema is also a very expensive operation in Spark, and can even be inaccurate if partitions contain different types.\n",
    "\n",
    "In the snippet below, we'll see that Koalas is consistent with Spark in being unable to have mixed type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/x5/f4r6ylss0k7dwh8c_0dmzd_40000gn/T/ipykernel_34170/2333051347.py\", line 3, in <module>\n",
      "    ps.from_pandas(mixed_df)\n",
      "  File \"/opt/miniconda3/envs/fugue-tutorials/lib/python3.7/site-packages/pyspark/pandas/namespace.py\", line 143, in from_pandas\n",
      "    return DataFrame(pobj)\n",
      "  File \"/opt/miniconda3/envs/fugue-tutorials/lib/python3.7/site-packages/pyspark/pandas/frame.py\", line 520, in __init__\n",
      "    internal = InternalFrame.from_pandas(pdf)\n",
      "  File \"/opt/miniconda3/envs/fugue-tutorials/lib/python3.7/site-packages/pyspark/pandas/internal.py\", line 1460, in from_pandas\n",
      "    ) = InternalFrame.prepare_pandas_frame(pdf)\n",
      "  File \"/opt/miniconda3/envs/fugue-tutorials/lib/python3.7/site-packages/pyspark/pandas/internal.py\", line 1533, in prepare_pandas_frame\n",
      "    spark_type = infer_pd_series_spark_type(reset_index[col], dtype)\n",
      "  File \"/opt/miniconda3/envs/fugue-tutorials/lib/python3.7/site-packages/pyspark/pandas/typedef/typehints.py\", line 329, in infer_pd_series_spark_type\n",
      "    return from_arrow_type(pa.Array.from_pandas(pser).type)\n",
      "  File \"pyarrow/array.pxi\", line 904, in pyarrow.lib.Array.from_pandas\n",
      "  File \"pyarrow/array.pxi\", line 302, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n",
      "  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowTypeError: Expected bytes, got a 'int' object\n"
     ]
    }
   ],
   "source": [
    "mixed_df = pd.DataFrame({\"a\": [None, \"test\", 1], \"b\": [1, 2, 3]})\n",
    "try:\n",
    "    ps.from_pandas(mixed_df)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistent Operation Behavior\n",
    "\n",
    "Some of the same API can have slightly different behavior. Take the following `assign()` statement. We create two columns `c` and `c_diff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>c_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>5</td>\n      <td>10.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.0</td>\n      <td>6</td>\n      <td>12.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     a  b     c  c_diff\n0  NaN  1   NaN     NaN\n1  NaN  2   NaN     NaN\n2  1.0  3   3.0     NaN\n3  1.0  4   4.0     1.0\n4  2.0  5  10.0     6.0\n5  2.0  6  12.0     2.0"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.assign(c=df['a'] * df[\"b\"], c_diff=lambda x: x['c'].diff())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same operation will actually fail in Koalas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/x5/f4r6ylss0k7dwh8c_0dmzd_40000gn/T/ipykernel_34170/10798204.py\", line 2, in <module>\n",
      "    kdf.assign(c=df['a'] * df[\"b\"], c_diff=lambda x: x['c'].diff())\n",
      "  File \"/opt/miniconda3/envs/fugue-tutorials/lib/python3.7/site-packages/pyspark/pandas/frame.py\", line 4903, in assign\n",
      "    return self._assign(kwargs)\n",
      "  File \"/opt/miniconda3/envs/fugue-tutorials/lib/python3.7/site-packages/pyspark/pandas/frame.py\", line 4916, in _assign\n",
      "    \"Column assignment doesn't support type \" \"{0}\".format(type(v).__name__)\n",
      "TypeError: Column assignment doesn't support type Series\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    kdf.assign(c=df['a'] * df[\"b\"], c_diff=lambda x: x['c'].diff())\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fails because the Pandas implementation can create `c_diff` after `c`, but for Koalas, they are created at the same time and `c_diff` can't reference `c`. The error does not make this immediately clear. There are a lot of caveats to watch out for when using Koalas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have shown some inconsistencies between Pandas and Koalas. Some of these are because Pandas was inherent not made for distribute compute. Some operations are harder to carry over, or in some cases, they don't make sense like `iloc`. These issues are not fixable because they are ingrained in the Pandas mindset. This is why Fugue deviates away from being another Pandas-like interface. Fugue considers Spark first, and then extends to local development rather than the other way around.\n",
    "\n",
    "Ultimately, changing one line in the import to bring code to Spark is an unrealistic expectation. There are a lot of tradeoffs to consider, and in order to support a Pandas interface, there needs to be tradeoffs in consistency and performance. If Koalas, is neither consistent with Pandas or Spark, how does it help?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('fugue-tutorials': conda)",
   "metadata": {
    "interpreter": {
     "hash": "131b24c7e1bb8763ab2b04d5b6d98a68c7b3a823a2a57c5722935f7690890f70"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}