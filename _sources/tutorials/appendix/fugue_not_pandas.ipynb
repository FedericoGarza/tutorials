{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Fugue Does NOT Want To Be Another Pandas-Like Framework\n",
    "\n",
    "Fugue fully utilizes Pandas for computing tasks, but **Fugue is NOT a Pandas-like computing framework, and it never wants to be.** In this article we are going to explain the reason for this critical design decision.\n",
    "\n",
    "## A Simple Example\n",
    "\n",
    "This is an example modified from a real piece of Pandas user code. Assume we have Pandas dataframes generated by this code:\n",
    "\n",
    "```python\n",
    "def gen(n):\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(dict(\n",
    "        a=np.random.choice([\"aa\",\"abcd\",\"xyzzzz\",\"tttfs\"],n),\n",
    "        b=np.random.randint(0,100,n),\n",
    "        c=np.random.choice([\"aa\",\"abcd\",\"xyzzzz\",\"tttfs\"],n),\n",
    "        d=np.random.randint(0,10000,n),\n",
    "    ))\n",
    "```\n",
    "\n",
    "The output has four columns with string and integer types. Here is the user's code:\n",
    "\n",
    "```python\n",
    "df.sort_values([\"a\", \"b\", \"c\", \"d\"]).drop_duplicates(subset=[\"a\", \"b\"], keep=\"last\")\n",
    "```\n",
    "\n",
    "Based on the code, the user want to firstly partition the dataframe by `a` and `b`,\n",
    "and in each group, the user wants to sort by `c` and `d` and then to get the last record\n",
    "of each group.\n",
    "\n",
    "\n",
    "### Configuration and Datasets\n",
    "\n",
    "* **Databricks runtime version:** 10.1 (Scala 2.12 Spark 3.2..0)\n",
    "* **Cluster:** 1 i3.xlarge driver instance and 8 i3.xlarge worker instances\n",
    "\n",
    "And we will use 4 different datasets: 1 million, 10 million, 20 million, and 30 million\n",
    "\n",
    "```python\n",
    "g1 = gen(1 * 1000 * 1000)\n",
    "df1 = spark.createDataFrame(g1).cache()\n",
    "df1.count()\n",
    "pdf1 = df1.to_pandas_on_spark()\n",
    "\n",
    "g10 = gen(10 * 1000 * 1000)\n",
    "df10 = spark.createDataFrame(g10).cache()\n",
    "df10.count()\n",
    "pdf10 = df10.to_pandas_on_spark()\n",
    "\n",
    "g20 = gen(20 * 1000 * 1000)\n",
    "df20 = spark.createDataFrame(g20).cache()\n",
    "df20.count()\n",
    "pdf20 = df20.to_pandas_on_spark()\n",
    "\n",
    "g30 = gen(30 * 1000 * 1000)\n",
    "df30 = spark.createDataFrame(g30).cache()\n",
    "df30.count()\n",
    "pdf30 = df30.to_pandas_on_spark()\n",
    "```\n",
    "\n",
    "### Comparison 1\n",
    "\n",
    "Let's firstly follow the user's original logic, and we will discuss the alternative solution later.\n",
    "\n",
    "In this [Databrick's article](https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html)\n",
    "the author claimed that Pandas users will be able to scale their workloads with one simple line change in the Spark 3.2 release.\n",
    "So we will first convert the Pandas dataframe to the Spark Pandas dataframe (and without any other change) to verify the result.\n",
    "\n",
    "On the other hand, in traditional Spark, a [window function solution](https://stackoverflow.com/a/33878701) is typical.\n",
    "So we will also add the window function solution to the comparison.\n",
    "\n",
    "To force the full execution of the statement and also to verify result consistency, at the end of each execution\n",
    "we will compute the sum of column `d` and print.\n",
    "\n",
    "Based on the output, the 3 solutions all have consistent result, meaning they have no correctness issue, now let's\n",
    "take a look at their speed:\n",
    "\n",
    "![Sort Dedup vs Window](../../images/pandas_like_1.png)\n",
    "\n",
    "* With a 32 core Spark cluster, both Spark solutions are significantly faster than\n",
    "  the single core Pandas solution\n",
    "* The window function solution is 30% to 50% faster than the Spark Pandas solution\n",
    "\n",
    "On a local machine, a global sort is a very popular technique that is often seen in Pandas code. And in certain\n",
    "scenarios it outperforms other methods. However\n",
    "the global sort operation in distributed computing is difficult and expensive. The performance depends on each\n",
    "specific computing framework's implementation. Spark Pandas has done an amazing job, but even so,\n",
    "it is still significantly slower than a window function.\n",
    "\n",
    "Rethinking about the problem we want to solve, a global sort on the entire dataset is not necessary.\n",
    "If convenience is the only thing important, then switching the Pandas backend to Spark Pandas may make sense.\n",
    "However the whole point of moving to Spark is to be more scalable and performant. Moving to window function that\n",
    "will sort inside each partition isn't overly complicated, but the performance advantage is significant.\n",
    "\n",
    "### Comparison 2\n",
    "\n",
    "In the second comparison, we simplify the original problem to not consider column `c`. We only need to remove\n",
    "`c` in `sort_values` to accommodate the change\n",
    "\n",
    "```python\n",
    "df.sort_values([\"a\", \"b\", \"d\"]).drop_duplicates(subset=[\"a\", \"b\"], keep=\"last\")\n",
    "```\n",
    "\n",
    "Again, it's intuitive and convenient and Spark Pandas can inherit this change too. However, this new problem\n",
    "actually means we want to group by `a` and `b` and get the max value of `d`. It can be a simple aggregation\n",
    "in big data. So in this comparison, we add the simple Spark aggregation approach.\n",
    "\n",
    "![Sort Dedup vs Window vs Aggregation](../../images/pandas_like_2.png)\n",
    "\n",
    "* The previous performance pattern stays the same\n",
    "* Spark aggregation takes ~1 sec regardless of data size\n",
    "\n",
    "So now, do you want to just remove column `c` for simplicity or do you want to rewrite the logic for performance?\n",
    "\n",
    "### Comparison 3\n",
    "\n",
    "Let's go back to the original logic where we still have 4 columns. By understanding the intention, we can have an alternative Pandas solution:\n",
    "\n",
    "```python\n",
    "df.groupby([\"a\", \"b\"]).apply(lambda df:df.sort_values([\"c\", \"d\"], ascending=[False, False]).head(1))\n",
    "```\n",
    "\n",
    "When testing on the 1 million dataset, the original logic takes `1.43 sec` while this new logic takes `2.2 sec`. This is probably\n",
    "one of the reasons the user chose the sort & dedup approach. On a small local dataset, a global sort seems to be faster.\n",
    "\n",
    "In this section, we are going to compare groupby-apply with sort-dedup on all datasets. In addition, this fits nicely\n",
    "with [Pandas UDF](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html) scenarios, so we will also\n",
    "compare with the Pandas UDF approach.\n",
    "\n",
    "To avoid duplication, we extract the lambda function:\n",
    "\n",
    "```python\n",
    "def largest(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.sort_values([\"c\",\"d\"], ascending=[False, False]).head(1)\n",
    "```\n",
    "\n",
    "Unfortunately, the first issue we encounter is that Spark Pandas can't take this function\n",
    "\n",
    "```python\n",
    "g1.groupby([\"a\", \"b\"]).apply(largest)  # g1 is a pandas dataframe, it works\n",
    "df1.groupby([\"a\", \"b\"]).applyInPandas(largest, schema=\"a string,b long,c string,d long\")  # Pandas UDF works\n",
    "pdf1.groupby([\"a\", \"b\"]).apply(largest)  # pdf1 is a spark pandas dataframe, it doesn't work\n",
    "```\n",
    "\n",
    "So for Spark Pandas we will need to use:\n",
    "\n",
    "```python\n",
    "pdf1.groupby([\"a\", \"b\"]).apply(lambda df: largest(df))\n",
    "```\n",
    "\n",
    "This breaks the claim that with an import change everything works out of the box.\n",
    "\n",
    "Now let's see the performance chart:\n",
    "\n",
    "![Sort Dedup vs Group Apply vs Pandas UDF](../../images/pandas_like_3.png)\n",
    "\n",
    "* For Pandas, when data size increases, groupby-apply has more performance advantage over sort-dedup\n",
    "* For Spark Pandas, groupby-apply is even slower than Pandas\n",
    "* Pandas UDF is the fastest Spark solution for this problem\n",
    "\n",
    "### Summary of Comparisons\n",
    "\n",
    "With the 3 comparisons we find out:\n",
    "\n",
    "* The convenience is at the cost of performance\n",
    "* Simply switching backend doesn't always work (not 100% consistent)\n",
    "* Simply switching backend can cause unexpected performance issues\n",
    "* Big data problems require different ways of thinking, users must learn and change their mindset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
