{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fugue on Databricks\n",
    "\n",
    "A lot of Databricks users use the `databricks-connect` library to execute Spark commands on a Databricks cluster instead of a local session. `databricks-connect` replaces the local installation of `pyspark` and makes `pyspark` code get executed on the cluster, allowing users to use the cluster directly from their IDE. \n",
    "\n",
    "In this tutorial, we will go through the following steps:\n",
    "1. \n",
    "\n",
    "For more information check the [Databricks](https://docs.databricks.com/dev-tools/databricks-connect.html) documentation for how to configure `databricks-connect`. Getting Databricks setup for each cloud provider will also vary a bit, but in general, you want to create a workspace. A workspace will look like the following:\n",
    "\n",
    "![Databricks workspace](https://cdn-images-1.medium.com/max/1600/1*YUF7X7cLLse1YDy2dTFh1Q.png)\n",
    "\n",
    "## Create cluster\n",
    "\n",
    "\n",
    "\n",
    "## Installing Fugue\n",
    "\n",
    "![Installing Fugue](https://cdn-images-1.medium.com/max/1600/1*z1AO5S17BxWFE1YGwj8RLQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fugue and databricks-connect\n",
    "\n",
    "Fugue helps solve this problem by allowing users to use the default `NativeExecutionEngine` for local development and testing. Users can use sampled files for local development, and then bring it to Spark when ready for larger tests. To demo this, we have a sample code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "numbers:long|words:str|reversed:str\n",
      "------------+---------+------------\n",
      "1           |hello    |olleh       \n",
      "2           |world    |dlrow       \n",
      "3           |apple    |elppa       \n",
      "4           |banana   |ananab      \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "data = pd.DataFrame({'numbers':[1,2,3,4], 'words':['hello','world','apple','banana']})\n",
    "\n",
    "# schema: *, reversed:str\n",
    "def reverse_word(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['reversed'] = df['words'].apply(lambda x: x[::-1])\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data)\n",
    "    df = df.transform(reverse_word)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this DataFrame is just 4 rows, but we should need to bring it to the Spark cluster if we were using `databricks-connect`. Here, we perform the test locally first and confirm that it works. After that, we can bring it to Spark with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "numbers:long|words:str|reversed:str\n",
      "------------+---------+------------\n",
      "1           |hello    |olleh       \n",
      "2           |world    |dlrow       \n",
      "3           |apple    |elppa       \n",
      "4           |banana   |ananab      \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Pandas DataFrame gets converted to Spark\n",
    "data = pd.DataFrame({'numbers':[1,2,3,4], 'words':['hello','world','apple','banana']})\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df(data)\n",
    "    df = df.transform(reverse_word)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No added work is needed. The `SparkExecutionEngine` imports `pyspark`, meaning that it will import the `databricks-connect` configuration under the hood and use the configured cluster. Fugue works with `databricks-connect` seamlessly, allowing for convenient switching between local development and a remote cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added Configuration\n",
    "\n",
    "Most `databricks-connect` users add additional Spark configurations on the cluster. If additional configruation is needed in local code, it can be provided with the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "spark_session = (SparkSession\n",
    "                 .builder\n",
    "                 .config(\"spark.executor.cores\",4)\n",
    "                 .config(\"fugue.dummy\",\"dummy\")\n",
    "                 .getOrCreate())\n",
    "\n",
    "engine = SparkExecutionEngine(spark_session, {\"additional_conf\":\"abc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fugue-sql on the Cluster\n",
    "\n",
    "Because Fugue-sql also just uses the `SparkExecutionEngine`, it can also be easily executed on a remote cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "numbers:long|words:str|reversed:str\n",
      "------------+---------+------------\n",
      "1           |hello    |olleh       \n",
      "2           |world    |dlrow       \n",
      "3           |apple    |elppa       \n",
      "4           |banana   |ananab      \n",
      "Total count: 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrames()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue_sql import fsql\n",
    "\n",
    "data = pd.DataFrame({'numbers':[1,2,3,4], 'words':['hello','world','apple','banana']})\n",
    "\n",
    "fsql(\"\"\"\n",
    "      SELECT *\n",
    "        FROM data\n",
    "      TRANSFORM USING reverse_word\n",
    "      PRINT\"\"\"\n",
    ").run(SparkExecutionEngine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\nrequire([\"codemirror/lib/codemirror\"]);\nfunction set(str) {\n    var obj = {}, words = str.split(\" \");\n    for (var i = 0; i < words.length; ++i) obj[words[i]] = true;\n    return obj;\n  }\nvar fugue_keywords = \"fill hash rand even presort persist broadcast params process output outtransform rowcount concurrency prepartition zip print title save append parquet csv json single checkpoint weak strong deterministic yield connect sample seed take sub callback dataframe file\";\nCodeMirror.defineMIME(\"text/x-fsql\", {\n    name: \"sql\",\n    keywords: set(fugue_keywords + \" add after all alter analyze and anti archive array as asc at between bucket buckets by cache cascade case cast change clear cluster clustered codegen collection column columns comment commit compact compactions compute concatenate cost create cross cube current current_date current_timestamp database databases data dbproperties defined delete delimited deny desc describe dfs directories distinct distribute drop else end escaped except exchange exists explain export extended external false fields fileformat first following for format formatted from full function functions global grant group grouping having if ignore import in index indexes inner inpath inputformat insert intersect interval into is items join keys last lateral lazy left like limit lines list load local location lock locks logical macro map minus msck natural no not null nulls of on optimize option options or order out outer outputformat over overwrite partition partitioned partitions percent preceding principals purge range recordreader recordwriter recover reduce refresh regexp rename repair replace reset restrict revoke right rlike role roles rollback rollup row rows schema schemas select semi separated serde serdeproperties set sets show skewed sort sorted start statistics stored stratify struct table tables tablesample tblproperties temp temporary terminated then to touch transaction transactions transform true truncate unarchive unbounded uncache union unlock unset use using values view when where window with\"),\n    builtin: set(\"date datetime tinyint smallint int bigint boolean float double string binary timestamp decimal array map struct uniontype delimited serde sequencefile textfile rcfile inputformat outputformat\"),\n    atoms: set(\"false true null\"),\n    operatorChars: /^[*\\/+\\-%<>!=~&|^]/,\n    dateSQL: set(\"time\"),\n    support: set(\"ODBCdotTable doubleQuote zerolessFloat\")\n  });\n\nCodeMirror.modeInfo.push( {\n            name: \"Fugue SQL\",\n            mime: \"text/x-fsql\",\n            mode: \"sql\"\n          } );\n\nrequire(['notebook/js/codecell'], function(codecell) {\n    codecell.CodeCell.options_default.highlight_modes['magic_text/x-fsql'] = {'reg':[/%%fsql/]} ;\n    Jupyter.notebook.events.on('kernel_ready.Kernel', function(){\n    Jupyter.notebook.get_cells().map(function(cell){\n        if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n    });\n  });\n",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fugue_notebook import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fsql spark\n",
    "SELECT *\n",
    "  FROM data\n",
    "TRANSFORM USING reverse_word\n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we have shown the painpoints in using `databricks-connect`. It slows down developer productitity and increases compute costs. We can solve both of these problems by toggling between Fugue's default `NativeExecutionEngine` and `SparkExecutionEngine`. Fugue's `SparkExecutionEngine` will seamlessly use whatever `pyspark` is configured for the user.\n",
    "\n",
    "Fugue also allows for additional configuration of the underlying frameworks. We showed the syntax for passing a `SparkSession` to the `SparkExecutionEngine`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}